{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BitLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=False, lora_rank=16):\n",
    "        super(BitLinear, self).__init__(in_features, out_features, bias)\n",
    "        self.gamma_forward = nn.Parameter(torch.randn(in_features), requires_grad=False)\n",
    "        self.beta = nn.Parameter(torch.randn(out_features), requires_grad=False)\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def binarize(self, W):\n",
    "        gamma = torch.sum(torch.abs(W)) / (W.shape[0] * W.shape[1])\n",
    "        W = W / (gamma + self.epsilon)\n",
    "        W_bin = torch.clamp(W, -1, 1).round()\n",
    "        return W + (W_bin - W).detach()\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.dim() == 1:\n",
    "            input = input.unsqueeze(0)\n",
    "\n",
    "        input_norm = F.layer_norm(input, (self.in_features,))\n",
    "        quant_scale = torch.max(torch.abs(input_norm), dim=1, keepdim=True).values\n",
    "        input_quant = torch.sign(input_norm) * (quant_scale / self.gamma_forward)\n",
    "        \n",
    "        binary_weight = self.binarize(self.weight)\n",
    "\n",
    "        output = torch.matmul(input_quant, binary_weight.t())\n",
    "        output = output * self.beta.expand_as(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulilioaica\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([1, 100])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "bitlayer = BitLinear(2500, 100)\n",
    "\n",
    "#train the model to output the same input\n",
    "train_loss = []\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(bitlayer.parameters(), lr=0.001)\n",
    "for i in range(1000):\n",
    "    input = torch.randn(2500)\n",
    "    output = bitlayer(input)\n",
    "    loss = criterion(output, torch.ones(100))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.append(loss.item())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAHHCAYAAABtF1i4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABBh0lEQVR4nO3deXxU1f3/8ffMJJkkQEIgkAAGg7iAooCgNO7WKCJfWixtXVACVi0KFkytggqIVqNWLbYiVCtQ64LiT3BhURqkVEWRJQjIWpZQJAlbViDLzPn9kclkhgTIkGRuwryej8c8yNy5y5lLYN7zOeeeazPGGAEAAFjEbnUDAABAaCOMAAAASxFGAACApQgjAADAUoQRAABgKcIIAACwFGEEAABYijACAAAsRRgBAACWIowAqGH48OFKTk4+pW2feOIJ2Wy2hm0QgNMaYQRoRmw2W50eS5cutbqplhg+fLhatmxpdTMABMjGvWmA5uOtt97ye/7mm29q8eLF+uc//+m3/Prrr1dCQsIpH6e8vFxut1tOpzPgbSsqKlRRUaHIyMhTPv6pGj58uD744AMVFxcH/dgATl2Y1Q0AUHd33HGH3/NvvvlGixcvrrH8WIcPH1Z0dHSdjxMeHn5K7ZOksLAwhYXxXwuAuqObBjjNXHPNNerRo4dWrVqlq666StHR0Xr00UclSR999JEGDhyojh07yul0qmvXrnrqqafkcrn89nHsmJGdO3fKZrPphRde0GuvvaauXbvK6XTqkksu0Xfffee3bW1jRmw2m0aPHq158+apR48ecjqduuCCC7Ro0aIa7V+6dKn69u2ryMhIde3aVX/7298afBzKnDlz1KdPH0VFRSk+Pl533HGH9uzZ47dOTk6ORowYoTPOOENOp1MdOnTQz3/+c+3cudO7zsqVK9W/f3/Fx8crKipKXbp00V133dVg7QRCBV9fgNPQgQMHNGDAAN1666264447vF02s2bNUsuWLZWenq6WLVtqyZIlmjhxogoLC/WnP/3ppPt95513VFRUpN/+9rey2Wx6/vnn9Ytf/ELbt28/aTXlyy+/1Icffqj7779frVq10l/+8hcNGTJE2dnZatu2rSRpzZo1uvHGG9WhQwdNnjxZLpdLTz75pNq1a1f/k+Ixa9YsjRgxQpdccokyMjKUm5url19+WV999ZXWrFmj1q1bS5KGDBmiDRs26IEHHlBycrLy8vK0ePFiZWdne5/fcMMNateuncaNG6fWrVtr586d+vDDDxusrUDIMACarVGjRplj/xlfffXVRpKZPn16jfUPHz5cY9lvf/tbEx0dbY4ePepdlpaWZs4880zv8x07dhhJpm3btubgwYPe5R999JGRZD755BPvskmTJtVokyQTERFhtm3b5l22du1aI8n89a9/9S4bNGiQiY6ONnv27PEu27p1qwkLC6uxz9qkpaWZFi1aHPf1srIy0759e9OjRw9z5MgR7/JPP/3USDITJ040xhhz6NAhI8n86U9/Ou6+5s6daySZ77777qTtAnBidNMApyGn06kRI0bUWB4VFeX9uaioSPv379eVV16pw4cPa9OmTSfd7y233KK4uDjv8yuvvFKStH379pNum5qaqq5du3qfX3TRRYqJifFu63K59K9//UuDBw9Wx44dveudffbZGjBgwEn3XxcrV65UXl6e7r//fr8BtgMHDlS3bt00f/58SZXnKSIiQkuXLtWhQ4dq3VdVBeXTTz9VeXl5g7QPCFXNKowsW7ZMgwYNUseOHWWz2TRv3ryAtj969KiGDx+uCy+8UGFhYRo8eHCt6y1dulQXX3yxnE6nzj77bM2aNavebQeCqVOnToqIiKixfMOGDbr55psVGxurmJgYtWvXzjv4taCg4KT77dy5s9/zqmByvA/sE21btX3Vtnl5eTpy5IjOPvvsGuvVtuxU7Nq1S5J03nnn1XitW7du3tedTqeee+45LVy4UAkJCbrqqqv0/PPPKycnx7v+1VdfrSFDhmjy5MmKj4/Xz3/+c82cOVOlpaUN0lYglDSrMFJSUqKePXtq6tSpp7S9y+VSVFSUfve73yk1NbXWdXbs2KGBAwfq2muvVVZWlsaOHau7775bn332WX2aDgSVbwWkSn5+vq6++mqtXbtWTz75pD755BMtXrxYzz33nCTJ7XafdL8Oh6PW5aYOMwTUZ1srjB07Vlu2bFFGRoYiIyM1YcIEde/eXWvWrJFUOSj3gw8+0PLlyzV69Gjt2bNHd911l/r06cOlxUCAmlUYGTBggP74xz/q5ptvrvX10tJSPfTQQ+rUqZNatGihfv36+U3+1KJFC02bNk333HOPEhMTa93H9OnT1aVLF7344ovq3r27Ro8erV/+8pf685//3BhvCQiapUuX6sCBA5o1a5bGjBmj//u//1Nqaqpft4uV2rdvr8jISG3btq3Ga7UtOxVnnnmmJGnz5s01Xtu8ebP39Spdu3bV73//e33++edav369ysrK9OKLL/qt85Of/ERPP/20Vq5cqbffflsbNmzQ7NmzG6S9QKhoVmHkZEaPHq3ly5dr9uzZ+v777/WrX/1KN954o7Zu3VrnfSxfvrxG1aR///5avnx5QzcXCKqqyoRvJaKsrEyvvvqqVU3y43A4lJqaqnnz5unHH3/0Lt+2bZsWLlzYIMfo27ev2rdvr+nTp/t1pyxcuFAbN27UwIEDJVXOy3L06FG/bbt27apWrVp5tzt06FCNqk6vXr0kia4aIECnzaW92dnZmjlzprKzs72D3x566CEtWrRIM2fO1DPPPFOn/eTk5NSYuTIhIUGFhYU6cuRIreVvoDm47LLLFBcXp7S0NP3ud7+TzWbTP//5zybVTfLEE0/o888/1+WXX6777rtPLpdLr7zyinr06KGsrKw67aO8vFx//OMfayxv06aN7r//fj333HMaMWKErr76at12223eS3uTk5P14IMPSpK2bNmi6667Tr/+9a91/vnnKywsTHPnzlVubq5uvfVWSdI//vEPvfrqq7r55pvVtWtXFRUV6fXXX1dMTIxuuummBjsnQCg4bcLIunXr5HK5dO655/otLy0t9c5hAISytm3b6tNPP9Xvf/97Pf7444qLi9Mdd9yh6667Tv3797e6eZKkPn36aOHChXrooYc0YcIEJSUl6cknn9TGjRvrdLWPVFntmTBhQo3lXbt21f3336/hw4crOjpazz77rB555BG1aNFCN998s5577jnvFTJJSUm67bbblJmZqX/+858KCwtTt27d9P7772vIkCGSKgewrlixQrNnz1Zubq5iY2N16aWX6u2331aXLl0a7JwAoaDZ3pvGZrNp7ty53iti3nvvPQ0dOlQbNmyoMVCuZcuWNcaIDB8+XPn5+TWuyLnqqqt08cUXa8qUKd5lM2fO1NixY+t0tQGAhjd48GBt2LAhoC5XAM3HaVMZ6d27t1wul/Ly8rxzH5yKlJQULViwwG/Z4sWLlZKSUt8mAqiDY7tDt27dqgULFigtLc3CVgFoTM0qjBQXF/uNqt+xY4eysrLUpk0bnXvuuRo6dKiGDRumF198Ub1799a+ffuUmZmpiy66yDsw7YcfflBZWZkOHjyooqIibz901cCzkSNH6pVXXtHDDz+su+66S0uWLNH777/vnQwJQOM666yzNHz4cJ111lnatWuXpk2bpoiICD388MNWNw1AY7Fw9teAffHFF0ZSjUdaWpoxpnKq54kTJ5rk5GQTHh5uOnToYG6++Wbz/fffe/dx5pln1rqPY4/Tq1cvExERYc466ywzc+bMIL5LILQNHz7cnHnmmcbpdJqYmBjTv39/s2rVKqubBaARNdsxIwAA4PRwWs0zAgAAmh/CCAAAsFSzGMDqdrv1448/qlWrVrLZbFY3BwAA1IExRkVFRerYsaPs9uPXP5pFGPnxxx+VlJRkdTMAAMAp2L17t84444zjvt4swkirVq0kVb6ZmJgYi1sDAADqorCwUElJSd7P8eNpFmGkqmsmJiaGMAIAQDNzsiEWDGAFAACWIowAAABLEUYAAIClCCMAAMBSAYeRZcuWadCgQerYsaNsNpvmzZtX522/+uorhYWFeW9KBwAAEHAYKSkpUc+ePTV16tSAtsvPz9ewYcN03XXXBXpIAABwGgv40t4BAwZowIABAR9o5MiRuv322+VwOAKqpgAAgNNbUMaMzJw5U9u3b9ekSZOCcTgAANCMNPqkZ1u3btW4ceP0n//8R2FhdTtcaWmpSktLvc8LCwsbq3kAAMBijVoZcblcuv322zV58mSde+65dd4uIyNDsbGx3gf3pQEA4PRlM8aYU97YZtPcuXM1ePDgWl/Pz89XXFycHA6Hd5nb7ZYxRg6HQ59//rl++tOf1tiutspIUlKSCgoKmA4eAIBmorCwULGxsSf9/G7UbpqYmBitW7fOb9mrr76qJUuW6IMPPlCXLl1q3c7pdMrpdDZm0wAAQBMRcBgpLi7Wtm3bvM937NihrKwstWnTRp07d9b48eO1Z88evfnmm7Lb7erRo4ff9u3bt1dkZGSN5VbYV1Sqo+Uuxbd0KirCcfINAABAgwt4zMjKlSvVu3dv9e7dW5KUnp6u3r17a+LEiZKkvXv3Kjs7u2Fb2Uju/edKXfn8F/py236rmwIAQMiq15iRYKlrn1Oghkz7Wqt2HdLf7uyj/hckNth+AQBA3T+/Q/reNHZb5Z9ud5PPYwAAnLZCOozYbJVphCwCAIB1QjqMOLxhhDQCAIBVQjqM2D3vnjACAIB1QjuMUBkBAMByIR1GvGNG3BY3BACAEBbSYcRRdTUNlREAACwT0mGkqpuGLAIAgHVCOoxUddO4SCMAAFgmpMOInW4aAAAsF9JhxGFn0jMAAKwW0mGkeswIaQQAAKuEdBjxZBG5KI0AAGCZkA4jdu5NAwCA5UI6jFSNGaGbBgAA64R0GLFxNQ0AAJYL6TBS1U3jYjp4AAAsE+JhpPJPKiMAAFgnxMMIY0YAALBaaIcRJj0DAMByoR1GmGcEAADLhXgYoZsGAACrEUZENw0AAFYijIiraQAAsFKIh5HKP12EEQAALBPaYcQ7HbzFDQEAIISFdBjxTgfPoBEAACwT0mHEwQBWAAAsF9JhhAGsAABYL8TDSOWfhBEAAKwT0mHERmUEAADLhXQYYdIzAACsF9JhxOF591xNAwCAdUI6jNBNAwCA9UI6jNBNAwCA9UI8jFT+SWUEAADrhHQYcXjSCGNGAACwTkiHERvdNAAAWC7gMLJs2TINGjRIHTt2lM1m07x58064/ocffqjrr79e7dq1U0xMjFJSUvTZZ5+dansbFN00AABYL+AwUlJSop49e2rq1Kl1Wn/ZsmW6/vrrtWDBAq1atUrXXnutBg0apDVr1gTc2IZWNYCVLAIAgHXCAt1gwIABGjBgQJ3XnzJlit/zZ555Rh999JE++eQT9e7dO9DDNyi7pzTiop8GAADLBBxG6svtdquoqEht2rQ57jqlpaUqLS31Pi8sLGyUttBNAwCA9YI+gPWFF15QcXGxfv3rXx93nYyMDMXGxnofSUlJjdIW5hkBAMB6QQ0j77zzjiZPnqz3339f7du3P+5648ePV0FBgfexe/fuRmlPVWXEUBkBAMAyQeummT17tu6++27NmTNHqampJ1zX6XTK6XQ2epuqKiMuwggAAJYJSmXk3Xff1YgRI/Tuu+9q4MCBwThkndBNAwCA9QKujBQXF2vbtm3e5zt27FBWVpbatGmjzp07a/z48dqzZ4/efPNNSZVdM2lpaXr55ZfVr18/5eTkSJKioqIUGxvbQG/j1Ng9UYxuGgAArBNwZWTlypXq3bu397Lc9PR09e7dWxMnTpQk7d27V9nZ2d71X3vtNVVUVGjUqFHq0KGD9zFmzJgGegunzs5dewEAsFzAlZFrrrnmhJWEWbNm+T1funRpoIcImqrp4JlnBAAA64T0vWkcnjBypNxtcUsAAAhdIR1GEmMrr9j5/n/5yi08anFrAAAITSEdRi7uHKf4lhEyRtqTf8Tq5gAAEJJCOozYbDa1dFYOm3EzbgQAAEuEdBiRuFkeAABWC/kw4mAWVgAALEUY8VRG3FxQAwCAJUI+jHB/GgAArBXyYaS6MkIYAQDACoQRTxipIIwAAGAJwghX0wAAYCnCCDfLAwDAUiEfRuyeM0BlBAAAa4R8GPEOYKUyAgCAJUI+jHgv7aUyAgCAJUI+jDCAFQAAaxFGGMAKAIClQj6MVN8oz+KGAAAQokI+jHCjPAAArEUYYTp4AAAsFfJhxM4AVgAALBXyYSSMMAIAgKVCPozYGTMCAIClQj6MOJgOHgAASxFGGMAKAIClQj6M0E0DAIC1Qj6MUBkBAMBaIR9GqIwAAGCtkA8jDqaDBwDAUoQROzfKAwDASiEfRrzdNIwZAQDAEiEfRphnBAAAaxFGbHTTAABgpZAPI9woDwAAa4V8GOFGeQAAWCvkwwiVEQAArBXyYcTBpGcAAFiKMMJ08AAAWCrgMLJs2TINGjRIHTt2lM1m07x58066zdKlS3XxxRfL6XTq7LPP1qxZs06hqY2jejp4ixsCAECICjiMlJSUqGfPnpo6dWqd1t+xY4cGDhyoa6+9VllZWRo7dqzuvvtuffbZZwE3tjFQGQEAwFphgW4wYMAADRgwoM7rT58+XV26dNGLL74oSerevbu+/PJL/fnPf1b//v0DPXyDYwArAADWavQxI8uXL1dqaqrfsv79+2v58uXH3aa0tFSFhYV+j8bCAFYAAKzV6GEkJydHCQkJfssSEhJUWFioI0eO1LpNRkaGYmNjvY+kpKRGa1/VdPB00wAAYI0meTXN+PHjVVBQ4H3s3r270Y5lpzICAIClAh4zEqjExETl5ub6LcvNzVVMTIyioqJq3cbpdMrpdDZ20yRVD2BlzAgAANZo9MpISkqKMjMz/ZYtXrxYKSkpjX3oOvFeTUNlBAAASwQcRoqLi5WVlaWsrCxJlZfuZmVlKTs7W1JlF8uwYcO8648cOVLbt2/Xww8/rE2bNunVV1/V+++/rwcffLBh3kE9URkBAMBaAYeRlStXqnfv3urdu7ckKT09Xb1799bEiRMlSXv37vUGE0nq0qWL5s+fr8WLF6tnz5568cUX9fe//71JXNYr+VxNQxgBAMASAY8Zueaaa2RO0KVR2+yq11xzjdasWRPooYKCeUYAALBWk7yaJpgcTAcPAIClCCNMBw8AgKVCPozQTQMAgLVCPoxUddNwaS8AANYI+TBi95wBKiMAAFgj5MMIN8oDAMBahBEGsAIAYKmQDyPeAaxURgAAsETIhxHvAFa3xQ0BACBEEUa4tBcAAEsRRjxhpIIwAgCAJQgjduYZAQDASiEfRuzctRcAAEuFfBjh0l4AAKxFGGHSMwAALBXyYYTp4AEAsFbIhxEGsAIAYC3CCANYAQCwVMiHEbu3MiIZqiMAAARdyIeRqsqIVBlIAABAcIV8GKmqjEh01QAAYIWQDyMOu29lhDACAECwEUZsVEYAALASYcSnMsLN8gAACD7CiG83DWEEAICgC/kw4pNFmBIeAAALhHwYsdls3kBCZQQAgOAL+TAiVXfVUBkBACD4CCOqrI5ITHoGAIAVCCMS3TQAAFiIMCLJ7jPXCAAACC7CiKrDCDOwAgAQfIQRSVWFEXppAAAIPsKIpKpOGiojAAAEH2FE1XfuJYsAABB8hBFVjxkxpBEAAIKOMCKfS3vJIgAABB1hRFLVqBHGjAAAEHynFEamTp2q5ORkRUZGql+/flqxYsUJ158yZYrOO+88RUVFKSkpSQ8++KCOHj16Sg1uDFWVEbIIAADBF3AYee+995Senq5JkyZp9erV6tmzp/r376+8vLxa13/nnXc0btw4TZo0SRs3btQbb7yh9957T48++mi9G99QmGcEAADrBBxGXnrpJd1zzz0aMWKEzj//fE2fPl3R0dGaMWNGret//fXXuvzyy3X77bcrOTlZN9xwg2677baTVlOCicoIAADWCSiMlJWVadWqVUpNTa3egd2u1NRULV++vNZtLrvsMq1atcobPrZv364FCxbopptuOu5xSktLVVhY6PdoTDYqIwAAWCYskJX3798vl8ulhIQEv+UJCQnatGlTrdvcfvvt2r9/v6644goZY1RRUaGRI0eesJsmIyNDkydPDqRp9VI1AytRBACA4Gv0q2mWLl2qZ555Rq+++qpWr16tDz/8UPPnz9dTTz113G3Gjx+vgoIC72P37t2N2kbGjAAAYJ2AKiPx8fFyOBzKzc31W56bm6vExMRat5kwYYLuvPNO3X333ZKkCy+8UCUlJbr33nv12GOPyW6vmYecTqecTmcgTauX6jEjhBEAAIItoMpIRESE+vTpo8zMTO8yt9utzMxMpaSk1LrN4cOHawQOh8Mhqel8+FePGbG4IQAAhKCAKiOSlJ6errS0NPXt21eXXnqppkyZopKSEo0YMUKSNGzYMHXq1EkZGRmSpEGDBumll15S79691a9fP23btk0TJkzQoEGDvKHEajaupgEAwDIBh5FbbrlF+/bt08SJE5WTk6NevXpp0aJF3kGt2dnZfpWQxx9/XDabTY8//rj27Nmjdu3aadCgQXr66acb7l3UE2NGAACwjs00lb6SEygsLFRsbKwKCgoUExPT4Pu/4c//1pbcYr1zTz9d1jW+wfcPAEAoquvnN/emkWRT1V17LW4IAAAhiDAixowAAGAlwogYMwIAgJUII5KqxtsSRgAACD7CiBgzAgCAlQgjqp6BlcoIAADBRxhR9QysZBEAAIKPMCIqIwAAWIkwIt+raSxuCAAAIYgwIt95RkgjAAAEG2FEPmNGLG4HAAChiDAixowAAGAlwogYMwIAgJUII2LMCAAAViKMqLoyQhYBACD4CCOqHsDKmBEAAIKPMCLfAazWtgMAgFBEGJE8t8mjMgIAgBUII6oeM8JEIwAABB9hRIwZAQDASoQRMWYEAAArEUZUPc8IlREAAIKPMCKfeUYsbgcAAKGIMCLfSc+IIwAABBthRD7dNAwaAQAg6Agjqr6apoIwAgBA0BFGVD1w9Y/zN9JVAwBAkBFGJO05dMT7c7mLMAIAQDARRuR/Sa/hmhoAAIKKMCLJ5TNWhF4aAACCizAi/zDCxGcAAAQXYUT+1RAuqAEAILgII5JchsoIAABWIYzIf7Iz47awIQAAhCDCiKiMAABgJcKIGMAKAICVCCPy76ZhACsAAMF1SmFk6tSpSk5OVmRkpPr166cVK1accP38/HyNGjVKHTp0kNPp1LnnnqsFCxacUoMbQ7nfPCOkEQAAgiks0A3ee+89paena/r06erXr5+mTJmi/v37a/PmzWrfvn2N9cvKynT99derffv2+uCDD9SpUyft2rVLrVu3boj2N4gKV/WoVSojAAAEV8Bh5KWXXtI999yjESNGSJKmT5+u+fPna8aMGRo3blyN9WfMmKGDBw/q66+/Vnh4uCQpOTm5fq1uYBUuxowAAGCVgLppysrKtGrVKqWmplbvwG5Xamqqli9fXus2H3/8sVJSUjRq1CglJCSoR48eeuaZZ+RyuerX8gZUwQBWAAAsE1BlZP/+/XK5XEpISPBbnpCQoE2bNtW6zfbt27VkyRINHTpUCxYs0LZt23T//fervLxckyZNqnWb0tJSlZaWep8XFhYG0syAVbiru2nIIgAABFejX03jdrvVvn17vfbaa+rTp49uueUWPfbYY5o+ffpxt8nIyFBsbKz3kZSU1KhtLPfppnExaAQAgKAKKIzEx8fL4XAoNzfXb3lubq4SExNr3aZDhw4699xz5XA4vMu6d++unJwclZWV1brN+PHjVVBQ4H3s3r07kGbWC900AAAEV0BhJCIiQn369FFmZqZ3mdvtVmZmplJSUmrd5vLLL9e2bdvk9ukK2bJlizp06KCIiIhat3E6nYqJifF7NKb4lk7vzxRGAAAIroC7adLT0/X666/rH//4hzZu3Kj77rtPJSUl3qtrhg0bpvHjx3vXv++++3Tw4EGNGTNGW7Zs0fz58/XMM89o1KhRDfcu6unNuy71/sw8IwAABFfAl/becsst2rdvnyZOnKicnBz16tVLixYt8g5qzc7Olt1enXGSkpL02Wef6cEHH9RFF12kTp06acyYMXrkkUca7l3U0/kdY9S2RYQOlJRRGQEAIMhsphmUAgoLCxUbG6uCgoJG67K55Ol/aV9RqRaOuVLdOzRutxAAAKGgrp/f3JvGw26r/JMBrAAABBdhxMNuq0wjZBEAAIKLMOJRFUaojAAAEFyEEQ+bt5vG2nYAABBqCCMeVEYAALAGYcSjagBrM7i4CACA0wphxKO6MmJxQwAACDGEEQ/vmBHSCAAAQUUY8aAyAgCANQgjHtXzjJBGAAAIJsKIB5f2AgBgDcKIB5f2AgBgDcKIR9WNhgkjAAAEF2HEg8oIAADWIIx42KrCiNvihgAAEGIIIx527wBWKiMAAAQTYcSDeUYAALAGYcSDe9MAAGANwoiHjcoIAACWIIx4MGYEAABrEEY8uLQXAABrEEY8HPaqe9NY3BAAAEIMYcTDRmUEAABLEEY87NwoDwAASxBGPBgzAgCANQgjHswzAgCANQgjHswzAgCANQgjHswzAgCANQgjHtybBgAAaxBGPKrCCGNGAAAILsKIh62qm4bSCAAAQUUY8aiqjLjIIgAABBVhxINLewEAsAZhxINJzwAAsAZhxIN5RgAAsAZhxIN5RgAAsAZhxKP60l6LGwIAQIghjHjYPWeCS3sBAAiuUwojU6dOVXJysiIjI9WvXz+tWLGiTtvNnj1bNptNgwcPPpXDNirGjAAAYI2Aw8h7772n9PR0TZo0SatXr1bPnj3Vv39/5eXlnXC7nTt36qGHHtKVV155yo1tTIwZAQDAGgGHkZdeekn33HOPRowYofPPP1/Tp09XdHS0ZsyYcdxtXC6Xhg4dqsmTJ+uss86qV4MbC9PBAwBgjYDCSFlZmVatWqXU1NTqHdjtSk1N1fLly4+73ZNPPqn27dvrN7/5TZ2OU1paqsLCQr9HY+NGeQAAWCOgMLJ//365XC4lJCT4LU9ISFBOTk6t23z55Zd644039Prrr9f5OBkZGYqNjfU+kpKSAmnmKbHRTQMAgCUa9WqaoqIi3XnnnXr99dcVHx9f5+3Gjx+vgoIC72P37t2N2MpKDiojAABYIiyQlePj4+VwOJSbm+u3PDc3V4mJiTXW/+9//6udO3dq0KBB3mVut7vywGFh2rx5s7p27VpjO6fTKafTGUjT6s1uZ8wIAABWCKgyEhERoT59+igzM9O7zO12KzMzUykpKTXW79atm9atW6esrCzv42c/+5muvfZaZWVlBaX7pa7opgEAwBoBVUYkKT09XWlpaerbt68uvfRSTZkyRSUlJRoxYoQkadiwYerUqZMyMjIUGRmpHj16+G3funVrSaqx3GoMYAUAwBoBh5FbbrlF+/bt08SJE5WTk6NevXpp0aJF3kGt2dnZstub38SuzDMCAIA1Ag4jkjR69GiNHj261teWLl16wm1nzZp1KodsdNybBgAAazS/EkYjqZoO3kU/DQAAQUUY8aCbBgAAaxBGPBjACgCANQgjHlWVEeYZAQAguAgjHjZvZYQwAgBAMBFGPOimAQDAGoQRDwawAgBgDcKIB/OMAABgDcKIB/emAQDAGoQRD8aMAABgDcKIB2NGAACwBmHEw26vGjNCGAEAIJgIIx7eeUbcFjcEAIAQQxjxoJsGAABrEEY8GMAKAIA1CCMe3JsGAABrEEY87NybBgAASxBGPOimAQDAGoQRD7vnTFAZAQAguAgjHnTTAABgDcKIB/OMAABgDcKIB/OMAABgDcKIR1U3DVkEAIDgIox4UBkBAMAahBEPGwNYAQCwBGHEg3lGAACwBmHEg+ngAQCwBmHEg8oIAADWIIx42BjACgCAJQgjHlRGAACwBmHEo3qeEdIIAADBRBjxYJ4RAACsQRjxsNFNAwCAJQgjHlRGAACwBmHEw27n3jQAAFiBMOJBZQQAAGsQRjyqxoy4GDQCAEBQEUY8qi/ttbghAACEmFMKI1OnTlVycrIiIyPVr18/rVix4rjrvv7667ryyisVFxenuLg4paamnnB9qzi4ay8AAJYIOIy89957Sk9P16RJk7R69Wr17NlT/fv3V15eXq3rL126VLfddpu++OILLV++XElJSbrhhhu0Z8+eeje+ITEdPAAA1rCZAKcc7devny655BK98sorkiS3262kpCQ98MADGjdu3Em3d7lciouL0yuvvKJhw4bV6ZiFhYWKjY1VQUGBYmJiAmlunf3wY6Fu+st/1K6VU989ltooxwAAIJTU9fM7oMpIWVmZVq1apdTU6g9ru92u1NRULV++vE77OHz4sMrLy9WmTZvjrlNaWqrCwkK/R2Oze84E08EDABBcAYWR/fv3y+VyKSEhwW95QkKCcnJy6rSPRx55RB07dvQLNMfKyMhQbGys95GUlBRIM0+JnatpAACwRFCvpnn22Wc1e/ZszZ07V5GRkcddb/z48SooKPA+du/e3ehtC/NMNFLhIowAABBMYYGsHB8fL4fDodzcXL/lubm5SkxMPOG2L7zwgp599ln961//0kUXXXTCdZ1Op5xOZyBNq7eIsMpcVupyB/W4AACEuoAqIxEREerTp48yMzO9y9xutzIzM5WSknLc7Z5//nk99dRTWrRokfr27XvqrW1EVWGkrMLNuBEAAIIooMqIJKWnpystLU19+/bVpZdeqilTpqikpEQjRoyQJA0bNkydOnVSRkaGJOm5557TxIkT9c477yg5Odk7tqRly5Zq2bJlA76V+nE6HN6fy11GEWE2C1sDAEDoCDiM3HLLLdq3b58mTpyonJwc9erVS4sWLfIOas3OzpbdXl1wmTZtmsrKyvTLX/7Sbz+TJk3SE088Ub/WN6Cqyogklbncfs8BAEDjCXieESsEY54Rl9uo66MLJEmrHk9V25bBHbMCAMDpplHmGTmdOew27xU1ZQxiBQAgaAgjPnwHsQIAgOAgjPggjAAAEHyEER8RDs9cI4QRAACChjDiwxnuqYwwZgQAgKAhjPioqozQTQMAQPAQRnxEhFVOfEY3DQAAwUMY8cEAVgAAgo8w4sNJNw0AAEFHGPFRPYDVZXFLAAAIHYQRH5HhlWNGDpcRRgAACBbCiI+Wzsr7BpaUVljcEgAAQgdhxEdVGCkupTICAECwEEZ8tKgKI0epjAAAECyEER8tnZVjRuimAQAgeAgjPryVkTLCCAAAwUIY8cEAVgAAgo8w4qMlY0YAAAg6wogPbzcNlREAAIKGMOLDWXVvGhfTwQMAECyEER+n+43y3G5jdRMAAKiBMOIj3HOjvPLTsDJSXFqhK5//Qg/NWWt1UwAA8EMY8eE8jSsjn6z9UXvyj+iDVf+zuile5S637pr1nV5ZstXqpgAALEQY8VFdGaE7Ixg+35CrJZvy9MLnW6xuCgDAQoQRH+EMYA2qI+XBvQfQF5vy9MtpX2vH/pKgHhcAcGKEER8RjupuGmOaTnXExcDTBjFi1ndaueuQ0t/PsropAAAfhBEfVWFEkiqaSADYX1yqS57+lx6bu87qppw2DhSXWd0EIOSVu9zaSZUSHoQRH1WX9kpNZxDrm8t36WBJmd7+NrvB9tkUKy0nuuz42+0H9Ic5a5V/mBABnC7ueXOlrnlhqT7fkGN1U9AEEEZ8hDts3p9Px8t7qzTF91buPn6bbnntG81Z9T9lLNgUxBYBaExLN++TJM38aqe1DUGTQBjxEeawy+7JI02lMtJQbD4/N8UBunW5gomBpwBweiKMHKPq8t6m+IFdH74f9eVNJGj5DhJuKm0CAAQfYeQYEafpXCMVPuGqqbw3t08YOd3CH4C6sdlOvg5Of4SRYzTl+9PUZ+BpmU8AaSpjRnzb1BTPNwAgOAgjx2jK96epzwe27/tpKlUI366Zpni+ATS+JjSlEyxEGDlGVWWktAl+Uy+tOPUZS/27aZrGeysLtOuogcq5lIUBoGkhjByj6vLepvKB7Tv/Rn0Ckl83TUXT+CriWxk5Hbppjpa79NCctVq0fq/VTQGCqj4zVvPlABJhpAZnmENS9X1Tvtq2X4/OXaeS0orjbrNofY7SZqxQXtHRBm+PbzWktPz06abZV1SqKZnVd+ttCm2qr7e+2aUPVv1PI99abXVTgKD5+3+26+KnFmtzTlGdt2lKt9tA00AYOUZCjFOStDe/MlgM/fu3eufbbL3x5Y7jbjPyrVX695Z9evGzhr/7rG81pD7dNE1tfMawGSv8BuQer02+3Uuqx/9fwfjP73+HjjT6MY5n0focpWRk6pvtByxrAxrfppxC5RY2/Jee+vjj/I06dLhcf5z/w0nXnTBvva59YakOlDCbMvydUhiZOnWqkpOTFRkZqX79+mnFihUnXH/OnDnq1q2bIiMjdeGFF2rBggWn1NhgSGoTLUn636HDfh+QOw+cfMKtHwsa/sPItxpSn24a3/dSYfGlvV9sytPGvYV+y44XRhpq7E4w7jXkW24+0fT2jWHkW6u0t+Co7ntrVVCPi+DZffCwbpzyH13zp6VWN6VWR8pO/mXpn9/s0o79Jfp/q/4XhBahOQk4jLz33ntKT0/XpEmTtHr1avXs2VP9+/dXXl5eret//fXXuu222/Sb3/xGa9as0eDBgzV48GCtX7++3o1vDGfERUmS/vH1zuN+yzTGqKS0Qit2HPTrvjn20tuS0opaP5SKSytq3GfF5TbeKoDvt3i/bpoKl4wxOloeeIXE95tIsCojW3KLtL+4tMbyR/7f9zWWHW/MiO97NfUojQRjTIpv8eX1/2y35F46hw6XB/2YCI5Vuw5JquxCPlx2/G7jpqrwaPXvZv6R6p+b4r2yEHxhgW7w0ksv6Z577tGIESMkSdOnT9f8+fM1Y8YMjRs3rsb6L7/8sm688Ub94Q9/kCQ99dRTWrx4sV555RVNnz69ns1veD06xUqSSspcuvON6orPh6v3aOnmfWoVGaY9h47U+k376/8e0I1TlunQ4TKF2e3ak19ZKbn2vHbaeeCw8g+XeT8sIhx29U2OkzFSUWm5cgqOKtxhV1KbaG3OKdKZbaN1tNylLbnF3v0/Pm+Dduwv1lFPtSQt5UztOHBYOQVH1CW+hfKKSpVXWKoj5S4lxUXpinPi1bF1lPbmH9Wn31cPqnzrm13afeiwyircMqayCuSw27XXs5/4lk65jdG+4lJFh4ephdOhI2UutXCGKa5FuNq2cMqoMjTZbDblHy5T4dEKxUaFS5Kiwh36dvsB/f3LHQqz2/T6sL4Kd9h1pNwlZ5hdeUU1A8q3Ow4qrkWEJKn4aIWcYXYdLnep0Oc/rYIj5fpq235t3Fuoru1ayhlmV7knxEWGO+Q2RnmFpUqOj5YzzKEyl1ul5W6VVriU7/MhvevAYW3KKVRpuVs/5h9RbFS4nOF2/e/QEbV0hslht8lhtyk2KlwOu01hdrtKK1yyyeatfriNkd1mU7jDLiMjt1v6budB7zEyFm7Sv7fs0+MDz5fNVvkfbrnLrXCHXWEOm+y1jNo7dolRZXCMcNhrDPI7Ua/T1lz/vnvfVY3xD3U2n6NWHcPm99zm/dnm2Zcxlc/LXW5VuIyiIhwypvKcuE3luXAbI4fdpnKXW9ERlefU5tlPVdurfofks9/9xaWKjQr3XmLv267q920876N6XzabZLfZaln3eGei9nNos9lkt1X+Wb3eMdt5lrncNQOyzed8VS/z3X/Npcdf1+a3bLvP7RC+2X5Andu0kNsYudyVv4sOe+XfkP/5rdx/hMOuCrdRhbvy33y4w+53l/LalLnc2l9cqviWTu/fX9U5rvo98g35Rytc+u++YhUcKff+X1D5Jauyjb7V5W151f+vFRwp17r/FXiO6dKhknIlx0fL5Zbmr9ur/2zdpzPbRGvAhR3UJb6Fwuw2Vf33G+75t1ThNrV2xdpq+XdmPL+bthNcnlffQbWn0itsZPTN9gNq4QxTzzNa168Bp6hdK6ciwx2WHNtmAuhMLysrU3R0tD744AMNHjzYuzwtLU35+fn66KOPamzTuXNnpaena+zYsd5lkyZN0rx587R27dpaj1NaWqrS0uoPrMLCQiUlJamgoEAxMTF1be4pMcbo0bnr9e6KhrtLLgAATd2H91+mizvHNeg+CwsLFRsbe9LP74AqI/v375fL5VJCQoLf8oSEBG3aVPsdVXNycmpdPyfn+LeNzsjI0OTJkwNpWoOx2WzK+MWFeubmHvrfoSPavr9EvZJaa/WuQ4oMd6iktEJREQ6Vu9xaufOQWkeHKzYqXBFhdm3cW6Q2LcJV4TZKjInU4TKX9heXql0rp1o6w9QlvoW25hYrqU20WkWGaeWuQ8opOKK8wlJ1bhMtt5H2FR9V66gIdWgdqegIh/YWHFXR0QqVV7hV5qr8Jv/9/wr0027tFdciQkfLXerUOkq7DhzWsq37dGlyG4WH2RUXHa6CI+X6Mf+oWkWGaVtesbbkFunKc9qprMKt/CNlauUMV0xUmMLsdh06XKav/3tAF3SM0Zlto1Vc6vLeNLDqG1T+4XLlHymTw273fst1G6mswqUz4iorOYfLXHIbo7YtnVq2ZZ/atIhQYkykKtxuOcMcKjpaLpvN5nfTu4gwuxJjImU83+paRITpSHllJeZwWYUOl7m0r6jUu05uYam6xLdQuMMmh90uh106WFwmh8OmjrGV58LIyBnmUESYXU7Pw0hak52vVpFhinBULmsdHaEyl1tHylzedUvKKnSkzKUwu11uY1ThNnJ65p+p+kZY9W3M7TaVVQNb5fcs36pPbFS4nGF2uY1kt1W+zwpX9bdTX8f7RuCw2/wH8R7zu1ql3OVW0dEKxUWH1/ptsLZv3L5H9f02Xfnc+Pxc/bzqm7HbXfntsqriVfXN3O6pUFSdH4e9cixBVeXEeNri963fpyJjjOTwuXu2t12eKlxl+6v34ft7eLxy//G+5dZWifK20/i8bqu5fuV7PX4lxvc8Hrusxrq1rVPLvqTKLl5JahUZ5m2D3ef9H3tuqrYvq3B7/r6qpi4wJ+2uNUZyGaNIz7+dqt8Dt/H/O/Btk01SZHhlJdVmq7z5aGV1sfL3o2qQd3zLCJWWu1VUWqEOsZHeY5ZWuGW3VY6VC3PY/LodO7WOUnFphYwxstttMqZygLvbSGF2W42/51p/Gzy/gFV/z7X9ahz793SqVZJT2azEM+4mOqJ+1YlTHa9fW8U2WALupgmG8ePHKz093fu8qjISTDabTUltor0DWq/t1r7GOtec57/s571Ovt+LfMpv3Ts0bpUHAIDmIKAwEh8fL4fDodzcXL/lubm5SkxMrHWbxMTEgNaXJKfTKafTGUjTAABAMxXQ1TQRERHq06ePMjMzvcvcbrcyMzOVkpJS6zYpKSl+60vS4sWLj7s+AAAILQF306SnpystLU19+/bVpZdeqilTpqikpMR7dc2wYcPUqVMnZWRkSJLGjBmjq6++Wi+++KIGDhyo2bNna+XKlXrttdca9p0AAIBmKeAwcsstt2jfvn2aOHGicnJy1KtXLy1atMg7SDU7O1t2e3XB5bLLLtM777yjxx9/XI8++qjOOecczZs3Tz169Gi4dwEAAJqtgC7ttUpdLw0CAABNR10/v7k3DQAAsBRhBAAAWIowAgAALEUYAQAAliKMAAAASxFGAACApQgjAADAUoQRAABgKcIIAACwVMDTwVuhapLYwsJCi1sCAADqqupz+2STvTeLMFJUVCRJSkpKsrglAAAgUEVFRYqNjT3u683i3jRut1s//vijWrVqJZvN1mD7LSwsVFJSknbv3s09bxoZ5zo4OM/BwXkODs5z8DTWuTbGqKioSB07dvS7ie6xmkVlxG6364wzzmi0/cfExPCLHiSc6+DgPAcH5zk4OM/B0xjn+kQVkSoMYAUAAJYijAAAAEuFdBhxOp2aNGmSnE6n1U057XGug4PzHByc5+DgPAeP1ee6WQxgBQAAp6+QrowAAADrEUYAAIClCCMAAMBShBEAAGCpkA4jU6dOVXJysiIjI9WvXz+tWLHC6iY1GxkZGbrkkkvUqlUrtW/fXoMHD9bmzZv91jl69KhGjRqltm3bqmXLlhoyZIhyc3P91snOztbAgQMVHR2t9u3b6w9/+IMqKiqC+VaalWeffVY2m01jx471LuM8N5w9e/bojjvuUNu2bRUVFaULL7xQK1eu9L5ujNHEiRPVoUMHRUVFKTU1VVu3bvXbx8GDBzV06FDFxMSodevW+s1vfqPi4uJgv5Umy+VyacKECerSpYuioqLUtWtXPfXUU373LuE8n5ply5Zp0KBB6tixo2w2m+bNm+f3ekOd1++//15XXnmlIiMjlZSUpOeff77+jTchavbs2SYiIsLMmDHDbNiwwdxzzz2mdevWJjc31+qmNQv9+/c3M2fONOvXrzdZWVnmpptuMp07dzbFxcXedUaOHGmSkpJMZmamWblypfnJT35iLrvsMu/rFRUVpkePHiY1NdWsWbPGLFiwwMTHx5vx48db8ZaavBUrVpjk5GRz0UUXmTFjxniXc54bxsGDB82ZZ55phg8fbr799luzfft289lnn5lt27Z513n22WdNbGysmTdvnlm7dq352c9+Zrp06WKOHDniXefGG280PXv2NN988435z3/+Y84++2xz2223WfGWmqSnn37atG3b1nz66admx44dZs6cOaZly5bm5Zdf9q7DeT41CxYsMI899pj58MMPjSQzd+5cv9cb4rwWFBSYhIQEM3ToULN+/Xrz7rvvmqioKPO3v/2tXm0P2TBy6aWXmlGjRnmfu1wu07FjR5ORkWFhq5qvvLw8I8n8+9//NsYYk5+fb8LDw82cOXO862zcuNFIMsuXLzfGVP7DsdvtJicnx7vOtGnTTExMjCktLQ3uG2jiioqKzDnnnGMWL15srr76am8Y4Tw3nEceecRcccUVx33d7XabxMRE86c//cm7LD8/3zidTvPuu+8aY4z54YcfjCTz3XffeddZuHChsdlsZs+ePY3X+GZk4MCB5q677vJb9otf/MIMHTrUGMN5bijHhpGGOq+vvvqqiYuL8/u/45FHHjHnnXdevdobkt00ZWVlWrVqlVJTU73L7Ha7UlNTtXz5cgtb1nwVFBRIktq0aSNJWrVqlcrLy/3Ocbdu3dS5c2fvOV6+fLkuvPBCJSQkeNfp37+/CgsLtWHDhiC2vukbNWqUBg4c6Hc+Jc5zQ/r444/Vt29f/epXv1L79u3Vu3dvvf76697Xd+zYoZycHL9zHRsbq379+vmd69atW6tv377edVJTU2W32/Xtt98G7800YZdddpkyMzO1ZcsWSdLatWv15ZdfasCAAZI4z42loc7r8uXLddVVVykiIsK7Tv/+/bV582YdOnTolNvXLG6U19D2798vl8vl95+zJCUkJGjTpk0Wtar5crvdGjt2rC6//HL16NFDkpSTk6OIiAi1bt3ab92EhATl5OR416nt76DqNVSaPXu2Vq9ere+++67Ga5znhrN9+3ZNmzZN6enpevTRR/Xdd9/pd7/7nSIiIpSWluY9V7WdS99z3b59e7/Xw8LC1KZNG861x7hx41RYWKhu3brJ4XDI5XLp6aef1tChQyWJ89xIGuq85uTkqEuXLjX2UfVaXFzcKbUvJMMIGtaoUaO0fv16ffnll1Y35bSze/dujRkzRosXL1ZkZKTVzTmtud1u9e3bV88884wkqXfv3lq/fr2mT5+utLQ0i1t3+nj//ff19ttv65133tEFF1ygrKwsjR07Vh07duQ8h7CQ7KaJj4+Xw+GoccVBbm6uEhMTLWpV8zR69Gh9+umn+uKLL3TGGWd4lycmJqqsrEz5+fl+6/ue48TExFr/DqpeQ2U3TF5eni6++GKFhYUpLCxM//73v/WXv/xFYWFhSkhI4Dw3kA4dOuj888/3W9a9e3dlZ2dLqj5XJ/p/IzExUXl5eX6vV1RU6ODBg5xrjz/84Q8aN26cbr31Vl144YW688479eCDDyojI0MS57mxNNR5baz/T0IyjERERKhPnz7KzMz0LnO73crMzFRKSoqFLWs+jDEaPXq05s6dqyVLltQo2/Xp00fh4eF+53jz5s3Kzs72nuOUlBStW7fO75d/8eLFiomJqfGhEKquu+46rVu3TllZWd5H3759NXToUO/PnOeGcfnll9e4PH3Lli0688wzJUldunRRYmKi37kuLCzUt99+63eu8/PztWrVKu86S5YskdvtVr9+/YLwLpq+w4cPy273/+hxOBxyu92SOM+NpaHOa0pKipYtW6by8nLvOosXL9Z55513yl00kkL70l6n02lmzZplfvjhB3Pvvfea1q1b+11xgOO77777TGxsrFm6dKnZu3ev93H48GHvOiNHjjSdO3c2S5YsMStXrjQpKSkmJSXF+3rVJac33HCDycrKMosWLTLt2rXjktOT8L2axhjOc0NZsWKFCQsLM08//bTZunWrefvtt010dLR56623vOs8++yzpnXr1uajjz4y33//vfn5z39e66WRvXv3Nt9++6358ssvzTnnnBPyl5z6SktLM506dfJe2vvhhx+a+Ph48/DDD3vX4TyfmqKiIrNmzRqzZs0aI8m89NJLZs2aNWbXrl3GmIY5r/n5+SYhIcHceeedZv369Wb27NkmOjqaS3vr469//avp3LmziYiIMJdeeqn55ptvrG5SsyGp1sfMmTO96xw5csTcf//9Ji4uzkRHR5ubb77Z7N27128/O3fuNAMGDDBRUVEmPj7e/P73vzfl5eVBfjfNy7FhhPPccD755BPTo0cP43Q6Tbdu3cxrr73m97rb7TYTJkwwCQkJxul0muuuu85s3rzZb50DBw6Y2267zbRs2dLExMSYESNGmKKiomC+jSatsLDQjBkzxnTu3NlERkaas846yzz22GN+l4pynk/NF198Uev/y2lpacaYhjuva9euNVdccYVxOp2mU6dO5tlnn613223G+Ex7BwAAEGQhOWYEAAA0HYQRAABgKcIIAACwFGEEAABYijACAAAsRRgBAACWIowAAABLEUYANAs2m03z5s2zuhkAGgFhBMBJDR8+XDabrcbjxhtvtLppAE4DYVY3AEDzcOONN2rmzJl+y5xOp0WtAXA6oTICoE6cTqcSExP9HlV36bTZbJo2bZoGDBigqKgonXXWWfrggw/8tl+3bp1++tOfKioqSm3bttW9996r4uJiv3VmzJihCy64QE6nUx06dNDo0aP9Xt+/f79uvvlmRUdH65xzztHHH3/sfe3QoUMaOnSo2rVrp6ioKJ1zzjk1whOApokwAqBBTJgwQUOGDNHatWs1dOhQ3Xrrrdq4caMkqaSkRP3791dcXJy+++47zZkzR//617/8wsa0adM0atQo3XvvvVq3bp0+/vhjnX322X7HmDx5sn7961/r+++/10033aShQ4fq4MGD3uP/8MMPWrhwoTZu3Khp06YpPj4+eCcAwKmr9632AJz20tLSjMPhMC1atPB7PP3008aYyrs4jxw50m+bfv36mfvuu88YY8xrr71m4uLiTHFxsff1+fPnG7vdbnJycowxxnTs2NE89thjx22DJPP44497nxcXFxtJZuHChcYYYwYNGmRGjBjRMG8YQFAxZgRAnVx77bWaNm2a37I2bdp4f05JSfF7LSUlRVlZWZKkjRs3qmfPnmrRooX39csvv1xut1ubN2+WzWbTjz/+qOuuu+6Ebbjooou8P7do0UIxMTHKy8uTJN13330aMmSIVq9erRtuuEGDBw/WZZdddkrvFUBwEUYA1EmLFi1qdJs0lKioqDqtFx4e7vfcZrPJ7XZLkgYMGKBdu3ZpwYIFWrx4sa677jqNGjVKL7zwQoO3F0DDYswIgAbxzTff1HjevXt3SVL37t21du1alZSUeF//6quvZLfbdd5556lVq1ZKTk5WZmZmvdrQrl07paWl6a233tKUKVP02muv1Wt/AIKDygiAOiktLVVOTo7fsrCwMO8g0Tlz5qhv37664oor9Pbbb2vFihV64403JElDhw7VpEmTlJaWpieeeEL79u3TAw88oDvvvFMJCQmSpCeeeEIjR45U+/btNWDAABUVFemrr77SAw88UKf2TZw4UX369NEFF1yg0tJSffrpp94wBKBpI4wAqJNFixapQ4cOfsvOO+88bdq0SVLllS6zZ8/W/fffrw4dOujdd9/V+eefL0mKjo7WZ599pjFjxuiSSy5RdHS0hgwZopdeesm7r7S0NB09elR//vOf9dBDDyk+Pl6//OUv69y+iIgIjR8/Xjt37lRUVJSuvPJKzZ49uwHeOYDGZjPGGKsbAaB5s9lsmjt3rgYPHmx1UwA0Q4wZAQAAliKMAAAASzFmBEC90dsLoD6ojAAAAEsRRgAAgKUIIwAAwFKEEQAAYCnCCAAAsBRhBAAAWIowAgAALEUYAQAAliKMAAAAS/1/dhYHchRQWjgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print the loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Any\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Positional encoding definition\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        self.pe = torch.zeros(max_len, 1, d_model)\n",
    "        self.pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = self.pe.transpose(0, 1)\n",
    "    def forward(self, x):\n",
    "        self.pe = self.pe.to(x.device)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_hidden, num_heads, seq_len, d_k) -> None:\n",
    "        super().__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_heads = num_heads\n",
    "        self.seq_len = seq_len\n",
    "        self.d_k = d_k\n",
    "\n",
    "        self.W_q = BitLinear(num_hidden, num_heads * num_hidden)\n",
    "        self.W_k = BitLinear(num_hidden, num_heads * num_hidden)\n",
    "        self.W_v = BitLinear(num_hidden, num_heads * num_hidden)\n",
    "        self.W_o = BitLinear(num_heads * num_hidden, num_hidden)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.mask = self.get_mask(self.seq_len)\n",
    "    \n",
    "    def get_mask(self, size):\n",
    "        device = next(self.parameters()).device\n",
    "        mask = torch.triu(torch.ones(size, size, device=device), diagonal=1)  \n",
    "        return mask.unsqueeze(0).unsqueeze(0)  \n",
    "\n",
    "    def forward(self, query, key, values, dropout=0.1, mask=None):\n",
    "        # Reshaping expanded to n_heads\n",
    "        query = self.W_q(query).view(-1, self.num_heads, self.seq_len, self.num_hidden)\n",
    "        key = self.W_k(key).view(-1, self.num_heads, self.seq_len, self.num_hidden)\n",
    "        values = self.W_v(values).view(-1, self.num_heads, self.seq_len, self.num_hidden)\n",
    "\n",
    "        # Q * K_T\n",
    "        QK_T = torch.matmul(query,  key.mT)\n",
    "\n",
    "        # QK_T / sqrt(dk)\n",
    "        QK_T = QK_T / math.sqrt(self.d_k)\n",
    "\n",
    "        if mask:\n",
    "            self.mask = self.mask.to(query.device)\n",
    "            QK_T = QK_T.masked_fill(self.mask == 1, float('-inf'))\n",
    "\n",
    "        # softmax(QK_T / sqrt(d_k)\n",
    "        attention_scores = self.softmax(QK_T)\n",
    "        \n",
    "        #dropout\n",
    "        attention_scores = self.dropout(attention_scores)\n",
    "        output = torch.matmul(attention_scores, values)  \n",
    "        # Reshape and apply output linear layer  \n",
    "        output = output.transpose(1, 2).contiguous().view(-1, self.seq_len, self.num_heads * self.num_hidden)  \n",
    "        output = self.W_o(output)  \n",
    "          \n",
    "        return output  \n",
    "\n",
    "# Feed forward definition\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, num_hidden, num_ffn_hidden) -> None:\n",
    "        super().__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_ffn_hidden = num_ffn_hidden\n",
    "\n",
    "        self.W_1 = BitLinear(num_hidden, num_ffn_hidden)\n",
    "        self.W_2 = BitLinear(num_ffn_hidden, num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.W_2(F.relu(self.W_1(x)))\n",
    "\n",
    "\n",
    "# Transformer definition\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, num_hidden, num_heads, seq_len) -> None:\n",
    "        super().__init__()\n",
    "        self.multihead_attention = MultiHeadAttention(num_hidden=num_hidden, num_heads=num_heads, seq_len=seq_len, d_k=1)\n",
    "        self.feed_forward = FeedForward(num_hidden=num_hidden, num_ffn_hidden=2*num_hidden)\n",
    "        self.layer_norm1 = nn.LayerNorm(num_hidden)\n",
    "        self.layer_norm2 = nn.LayerNorm(num_hidden)\n",
    "    \n",
    "    def forward(self, input_with_pos):\n",
    "        #attention \n",
    "        x = self.multihead_attention(input_with_pos, input_with_pos, input_with_pos)\n",
    "        \n",
    "        #add and norm\n",
    "        x = x + input_with_pos\n",
    "        x = self.layer_norm1(x)\n",
    "\n",
    "        # add norm \n",
    "        x_final = self.feed_forward(x)\n",
    "        x = x + x_final\n",
    "\n",
    "        x = self.layer_norm2(x)\n",
    "        return x\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_layers, n_heads, seq_len, num_hidden) -> None:\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.decoders = nn.ModuleList([TransformerDecoderLayer(num_hidden, n_heads, seq_len) for i in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, encoder_output):\n",
    "        for layer in self.decoders:\n",
    "            x = layer(x, encoder_output)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, n_heads, seq_len, num_hidden) -> None:\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.encoders = nn.ModuleList([TransformerEncoderLayer(num_hidden, n_heads, seq_len) for i in range(num_layers)])\n",
    "    def forward(self, x):\n",
    "        for layer in self.encoders:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, num_hidden, num_heads, seq_len) -> None:\n",
    "        super().__init__()\n",
    "        self.multihead_attention_masked = MultiHeadAttention(num_hidden=num_hidden, num_heads=num_heads, seq_len=seq_len, d_k=1)\n",
    "        self.multihead_attention = MultiHeadAttention(num_hidden=num_hidden, num_heads=num_heads, seq_len=seq_len, d_k=1)\n",
    "        \n",
    "        self.feed_forward = FeedForward(num_hidden=num_hidden, num_ffn_hidden=2*num_hidden)\n",
    "        self.layer_norm1 = nn.LayerNorm(num_hidden)\n",
    "        self.layer_norm2 = nn.LayerNorm(num_hidden)\n",
    "        self.layer_norm3 = nn.LayerNorm(num_hidden)\n",
    "    \n",
    "    def forward(self, output_with_pos, encoder_output):\n",
    "        # masked attention\n",
    "        x = self.multihead_attention_masked(output_with_pos, output_with_pos, output_with_pos, mask=True)\n",
    "        #add and norm\n",
    "        x = x + output_with_pos\n",
    "        x = self.layer_norm1(x)\n",
    "\n",
    "        # attention\n",
    "        x_attention = self.multihead_attention(encoder_output, encoder_output, x)\n",
    "\n",
    "        #add and norm\n",
    "        x = x + x_attention\n",
    "        x = self.layer_norm2(x)\n",
    "\n",
    "        #feed forward\n",
    "        x_forward = self.feed_forward(x)\n",
    "\n",
    "        #add and norm\n",
    "        x = x + x_forward\n",
    "        x = self.layer_norm3(x)\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder_layers_num, decoder_layers_num, num_hidden, num_heads, seq_len, vocab_size, embedding_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(encoder_layers_num, num_heads, seq_len, num_hidden)\n",
    "        self.decoder = TransformerDecoder(decoder_layers_num, num_heads, seq_len, num_hidden)\n",
    "        self.pos_enc = PositionalEncoding(embedding_dim, max_len=seq_len)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        #embeddings\n",
    "        x = self.embedding(x)\n",
    "        y = self.embedding(y)\n",
    "\n",
    "        #pos encodings\n",
    "        x = self.pos_enc(x)\n",
    "        y = self.pos_enc(y)\n",
    "\n",
    "        #forward pass\n",
    "        enc_output = self.encoder(x)\n",
    "        dec_output = self.decoder(y, enc_output)\n",
    "        output = self.linear(dec_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(2, 2, 2500, 4, 128, 2000, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[2500], expected input with shape [*, 2500], but got input of size[1, 128, 2048]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2000\u001b[39m, (\u001b[38;5;241m128\u001b[39m,))\n\u001b[1;32m----> 7\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, \u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m      9\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[139], line 189\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    186\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_enc(y)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m#forward pass\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m enc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m dec_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(y, enc_output)\n\u001b[0;32m    191\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(dec_output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[139], line 132\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoders:\n\u001b[1;32m--> 132\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[139], line 101\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, input_with_pos)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_with_pos):\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m#attention \u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultihead_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_with_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_with_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_with_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m#add and norm\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m input_with_pos\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[139], line 48\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, query, key, values, dropout, mask)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, key, values, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# Reshaping expanded to n_heads\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_hidden)\n\u001b[0;32m     49\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_k(key)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_hidden)\n\u001b[0;32m     50\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_v(values)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_hidden)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[117], line 23\u001b[0m, in \u001b[0;36mBitLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m input_norm \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m quant_scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(torch\u001b[38;5;241m.\u001b[39mabs(input_norm), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     25\u001b[0m input_quant \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msign(input_norm) \u001b[38;5;241m*\u001b[39m (quant_scale \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma_forward)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\functional.py:2543\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2541\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[0;32m   2542\u001b[0m     )\n\u001b[1;32m-> 2543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given normalized_shape=[2500], expected input with shape [*, 2500], but got input of size[1, 128, 2048]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "#train the model to output the same input\n",
    "train_loss = []\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "for i in tqdm(range(1000)):\n",
    "    input = torch.randint(0, 2000, (128,))\n",
    "    output = model(input, input)\n",
    "    loss = criterion(output, input)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.append(loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
